{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IPython notebook for generating custom train/test split. If possible, use test split included in data_splits directory for consistency. \n",
    "#### Requires hardcoded paths to 50% clusters, as well as SwissProt fasta file. Uniref50 clusters filtered to Swissprot available on request. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def nested_len(cluster_list):\n",
    "    return sum(len(x) for x in cluster_list)\n",
    "\n",
    "def JS_divergence(training_counts, testing_counts):\n",
    "    P = training_counts / np.linalg.norm(training_counts, ord=1)\n",
    "    Q = testing_counts / np.linalg.norm(testing_counts, ord=1)\n",
    "    M = 0.5 * (P + Q)\n",
    "    return 0.5 * (scipy.stats.entropy(P, M) + scipy.stats.entropy(Q, M))\n",
    "\n",
    "def load_clusters(clusters_path, protein_ids):\n",
    "    cluster_list = []\n",
    "    protein_count = 0\n",
    "    reader = pd.read_csv(clusters_path, sep='\\t', lineterminator='\\n', chunksize = 10000)\n",
    "    chunk_count = 0\n",
    "    for chunk in reader:\n",
    "        chunk_count += 1\n",
    "        column = chunk[\"Cluster members\"]\n",
    "        for id_list in column:\n",
    "            unfiltered_cluster = id_list.split(\"; \")\n",
    "            cluster = []\n",
    "            for prot_id in unfiltered_cluster:\n",
    "                if(prot_id in protein_ids):\n",
    "                    cluster.append(prot_id)\n",
    "            if(len(cluster) > 0):\n",
    "                cluster_list.append(cluster)\n",
    "    return cluster_list\n",
    "\n",
    "def build_random_split(cluster_list, train_fraction=0.9):\n",
    "    protein_count = nested_len(cluster_list)\n",
    "    random.shuffle(cluster_list)\n",
    "    i = 0\n",
    "    training_size = 0\n",
    "    training_set = []\n",
    "    while (training_size < train_fraction*protein_count):\n",
    "        cluster = cluster_list[i]\n",
    "        training_set.extend(cluster)\n",
    "        training_size += len(cluster)\n",
    "        i += 1\n",
    "    testing_set = []\n",
    "    while (i < len(cluster_list)):\n",
    "        cluster = cluster_list[i]\n",
    "        testing_set.extend(cluster)\n",
    "        i += 1\n",
    "    return training_set, testing_set\n",
    "\n",
    "def build_random_cluster_split(cluster_list, train_fraction=0.9):\n",
    "    protein_count = nested_len(cluster_list)\n",
    "    random.shuffle(cluster_list)\n",
    "    i = 0\n",
    "    training_size = 0\n",
    "    training_set = []\n",
    "    while (training_size < train_fraction*protein_count):\n",
    "        cluster = cluster_list[i]\n",
    "        training_set.append(cluster)\n",
    "        training_size += len(cluster)\n",
    "        i += 1\n",
    "    testing_set = []\n",
    "    while (i < len(cluster_list)):\n",
    "        cluster = cluster_list[i]\n",
    "        testing_set.append(cluster)\n",
    "        i += 1\n",
    "    return training_set, testing_set\n",
    "\n",
    "def enforce_threshold(annotation_count_dict, namespace, threshold):\n",
    "    term_list = [] \n",
    "    for term, count in annotation_count_dict.items():\n",
    "        if(godag[term].namespace == namespace or namespace == \"all\"):\n",
    "            term_list.append((count, term))\n",
    "    term_list.sort(reverse=True)\n",
    "    i = 0\n",
    "    while i < len(term_list) and term_list[i] >= threshold:\n",
    "        i += 1\n",
    "    return [x[1] for x in term_list[:i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 proteins processed\n",
      "10000 proteins processed\n",
      "20000 proteins processed\n",
      "30000 proteins processed\n",
      "40000 proteins processed\n",
      "50000 proteins processed\n",
      "60000 proteins processed\n",
      "70000 proteins processed\n",
      "80000 proteins processed\n",
      "90000 proteins processed\n",
      "100000 proteins processed\n",
      "110000 proteins processed\n",
      "120000 proteins processed\n",
      "130000 proteins processed\n",
      "140000 proteins processed\n",
      "150000 proteins processed\n",
      "160000 proteins processed\n",
      "170000 proteins processed\n",
      "180000 proteins processed\n",
      "190000 proteins processed\n",
      "200000 proteins processed\n",
      "210000 proteins processed\n",
      "220000 proteins processed\n",
      "230000 proteins processed\n",
      "240000 proteins processed\n",
      "250000 proteins processed\n",
      "260000 proteins processed\n",
      "270000 proteins processed\n",
      "280000 proteins processed\n",
      "290000 proteins processed\n",
      "300000 proteins processed\n",
      "310000 proteins processed\n",
      "320000 proteins processed\n",
      "330000 proteins processed\n",
      "340000 proteins processed\n",
      "350000 proteins processed\n",
      "360000 proteins processed\n",
      "370000 proteins processed\n",
      "380000 proteins processed\n",
      "390000 proteins processed\n",
      "400000 proteins processed\n",
      "410000 proteins processed\n",
      "420000 proteins processed\n",
      "430000 proteins processed\n",
      "440000 proteins processed\n",
      "450000 proteins processed\n",
      "460000 proteins processed\n",
      "470000 proteins processed\n",
      "480000 proteins processed\n",
      "490000 proteins processed\n",
      "500000 proteins processed\n",
      "510000 proteins processed\n",
      "520000 proteins processed\n",
      "530000 proteins processed\n",
      "540000 proteins processed\n",
      "550000 proteins processed\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "from Bio import SeqIO\n",
    "sequences = []\n",
    "prot_ids = []\n",
    "with gzip.open(\"../data/swissprot/uniprot_sprot.fasta.gz\", 'rt') as fp:\n",
    "    seqs = SeqIO.parse(fp, 'fasta')\n",
    "    count = 0\n",
    "    for rec in seqs:\n",
    "        seq_id = rec.id\n",
    "        if('|' in seq_id):\n",
    "            seq_id = rec.id.split('|')[1]\n",
    "        seq = rec.seq\n",
    "        sequences.append(str(seq.lower()))\n",
    "        prot_ids.append(seq_id)\n",
    "        if(count % 10000 == 0):\n",
    "            print(f\"{count} proteins processed\")\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = load_clusters(\"../data/uniref-reviewed+identity_0.5.tab\", set(prot_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_random_cluster_split(cluster_list, train_fraction=0.7, val_fraction=0.15):\n",
    "    protein_count = nested_len(cluster_list)\n",
    "    random.shuffle(cluster_list)\n",
    "    i = 0\n",
    "    training_size = 0\n",
    "    training_set = []\n",
    "    while (training_size < train_fraction*protein_count):\n",
    "        cluster = cluster_list[i]\n",
    "        training_set.extend(cluster)\n",
    "        training_size += len(cluster)\n",
    "        i += 1\n",
    "    validation_size = 0\n",
    "    validation_set = []\n",
    "    while (validation_size < val_fraction*protein_count):\n",
    "        cluster = cluster_list[i]\n",
    "        validation_set.extend(cluster)\n",
    "        validation_size += len(cluster)\n",
    "        i += 1\n",
    "    testing_set = []\n",
    "    while (i < len(cluster_list)):\n",
    "        cluster = cluster_list[i]\n",
    "        testing_set.extend(cluster)\n",
    "        i += 1\n",
    "    return training_set, validation_set, testing_set\n",
    "\n",
    "training_set, validation_set, testing_set = build_random_cluster_split(clusters, train_fraction=0.7, val_fraction=0.15)\n",
    "splits = (training_set, validation_set, testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts = [\"training\", \"validation\", \"testing\"]\n",
    "for ttt, ttt_set in zip(sts, splits):\n",
    "        with open(f\"../data/test_split_bootstrap2/test_split/{ttt}_ids.txt\", \"w\") as f:\n",
    "            f.writelines([x+\"\\n\" for x in ttt_set])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "408269ca62060850a8947a0d7a551465652cb7138ef02f7a1c955677fbaf7202"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('research')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
